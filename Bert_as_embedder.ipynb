{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert-as-embedder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+qkN16rbK/rSVHpg38gC/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathluva/bert-as-embedder/blob/main/Bert_as_embedder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3ypw_VCE_Fn"
      },
      "source": [
        "#import dependencies\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUEr6V-VFVSG",
        "outputId": "2e20992e-9df4-4c3b-ef3f-74cf5c133d2e"
      },
      "source": [
        "#use ! for terminal commands\n",
        "!pip install bert-for-tf2 #tensorflow2 light version\n",
        "!pip install sentencepiece #required for BERT-tf2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/a1/acb891630749c56901e770a34d6bac8a509a367dd74a05daf7306952e910/bert-for-tf2-0.14.9.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 26.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 22.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/e0/4f663d8abf83c8084b75b995bd2ab3a9512ebc5b97206fde38cef906ab07/py-params-0.10.2.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-cp37-none-any.whl size=30535 sha256=638a48c44e0022722c64a2d92a0a965bb6710d8f9c1fd64b1bf63f256d13f302\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/04/ee/347bd9f5b821b637c76411d280271a857aece00358896a230f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-cp37-none-any.whl size=7912 sha256=40315e52bf766149f39b6468cf6bba59d2794abdbe4c3deb4a9102cb18171b89\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/4a/70/ff12450229ff1955abf01f365051d4faae1c20aef53ab4cf09\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp37-none-any.whl size=19472 sha256=3e23e7c862e96d731eda46fd693249c0e59b78b1afbe14a6773503adb8c2ce7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 19.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrNuhRxrFZJM",
        "outputId": "22b7165f-ee85-4071-8eca-a6011cbdb697"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x #only available in Google colab\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub #used to import the weights from BERT\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert #installed in previous step"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x #only available in Google colab`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmyCFWZ3FdN0",
        "outputId": "c0761640-c158-40ff-c855-695c66fe3373"
      },
      "source": [
        "#load files, data preprocessing\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb1heruuFg-U"
      },
      "source": [
        "#label columns\n",
        "#latin1 is common for western languages\n",
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/trainingandtestdata.zip (Unzipped Files)/training.1600000.processed.noemoticon.csv\", \n",
        "    header = None,\n",
        "    names = cols,\n",
        "    engine = \"python\",\n",
        "    encoding = \"latin1\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vnUU7YpFqQW"
      },
      "source": [
        "#axis1 column data\n",
        "#without inplace=True, it would be required to write data = data.drop(\"...\")\n",
        "data.drop([\"id\", \"date\",\"query\", \"user\"], axis = 1, inplace = True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "oI0q8xcHFwEO",
        "outputId": "77119084-837a-4d46-859c-a2a99f822f6f"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-RmF3OFFyY_"
      },
      "source": [
        "#cleaning\n",
        "#r is regEX (regexr.com for more documentation)\n",
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text() #tweets are not usuable as standard string, need BS to extract string\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ',tweet)#anything behind @symbol with empty space, apply to tweet\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)#? means the s can be there or not\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?]\", ' ', tweet) #keep only standard characters\n",
        "    tweet = re.sub(r\" +\", ' ', tweet) #replace multiple sequences of white space with only one white space\n",
        "    return tweet"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvuRJmC0F1w_"
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLXFbtqOF4D3"
      },
      "source": [
        "#process sentiment\n",
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels ==4] =1 #data is using 0 and 4, replace 4 with standard 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFSl7NfDHbxP"
      },
      "source": [
        "#create BERT layer to have access to metadata for the tokenizer(like vocab size).\n",
        "#call BERT as a layer, hub is where all pretrained models are located\n",
        "#trainable = False bc we are not fine-tuning the weights\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable = False) \n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() #way to have acces to vocab\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuOpIOSaHuYc"
      },
      "source": [
        "We only use the first sentence for BERT inputs so we add the CLS token at the beginning and the SEP token at the end of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxXGG7vzF9Bo"
      },
      "source": [
        "#the first layer is a BERT layer so\n",
        "#make inputs suitable for BERT\n",
        "def encode_sentence(sent):\n",
        "    return [\"CLS\"] + tokenizer.tokenize(sent) + [\"SEP\"]\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuAWIS-7IWXf"
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPG--ueVI6B3"
      },
      "source": [
        "Dataset Creation\n",
        "\n",
        "We need to create 3 different inputs for each sentence:\n",
        "1.  The tokenize version of the sentence created in data_inputs\n",
        "2. Mask layer: 1 for standard tokens and 0 for padding\n",
        "3. Segment input: sequence of 0 and 1's , 0  correspond to first sentence and 1 for second sentence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGMuJJFFKF3k"
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_ids_to_tokens(tokens)\n",
        "\n",
        "\n",
        "#padding mask, compare each element of token with the string \"[PAD]\"\n",
        "#not_equal will give us a 1 when not using [PAD] token\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "\n",
        "#determine if 1st or 2nd sentence\n",
        "#first sentence is 0 \"current_id\", after [SEP] turn 0 to 1\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok ==\"[SEP]\":\n",
        "            current_seg_id = 1- current_seg_id  #once we get to a second [SEP] it will turn back to 0, for sentence pair options in BERT\n",
        "        return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrcF-KnANkbM"
      },
      "source": [
        "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible.  For that, we sort sentences by length, apply padded_batches and then shuffle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjVdWZyANiDG"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)] for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key = lambda x:x[2]) #use 3rd element to sort list\n",
        "sorted_all = [([get_ids(sent_lab[0]), \n",
        "              get_mask(sent_lab[0]),\n",
        "              get_segments(sent_lab[0])],\n",
        "               sent_lab[1]) for sent_lab in data_with_len if sent_lab[2] >7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg5m83BIV98y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}